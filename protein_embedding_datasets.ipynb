{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing ProteinEmbedding and Dataset formation\n",
    "\n",
    "The _ProteinEmbedding_ object can store protein sequences in combination with embedding and vector representations,\n",
    "which can then be manipulated by implementing standard operations from the __skbio__ library. \n",
    "\n",
    "To demonstrate this process, we will read in a list of protein sequences, then attempt to create a dataset from our \n",
    "embeddings to be streamed in with the standard skbio.read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from skbio.embedding import ProteinEmbedding\n",
    "from skbio.sequence import Protein\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import skbio\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings\n",
    "\n",
    "This function will take the inputted protein sequences and feed it through an embedding model (prot-t5), \n",
    "outputting the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_protein_t5_embedding(sequence, model_name, tokenizer_name):\n",
    "    import torch\n",
    "    from transformers import T5Tokenizer, T5EncoderModel\n",
    "    # (In case we want to use ONNX model)\n",
    "    # from optimum.onnxruntime import ORTModel\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
    "    model = T5EncoderModel.from_pretrained(model_name)\n",
    "\n",
    "    # convert sequence to formatted list of strings\n",
    "    seq_list = []\n",
    "    seq_list.append(sequence)\n",
    "    seqs = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", str(seq)))) for seq in seq_list]\n",
    "    \n",
    "    # tokenize sequences and pad up to the longest sequence in the batch\n",
    "    ids = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "    input_ids = torch.tensor(ids['input_ids'])\n",
    "    attention_mask = torch.tensor(ids['attention_mask'])\n",
    "\n",
    "    # generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "    return ProteinEmbedding(embedding_repr.last_hidden_state.detach().cpu().numpy()[0][:-1], sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse arguments\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Modify the default values of the arguments to match the desired values\n",
    "parser.add_argument(\"--n_sequences\", type=int, default=20)\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Rostlab/prot_t5_xl_uniref50\")\n",
    "parser.add_argument(\"--tokenizer_name\", type=str, default=\"Rostlab/prot_t5_xl_uniref50\")\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#need str like object\n",
    "sequence_list = skbio.read('test.fa', format='fasta', constructor=Protein)\n",
    "#sequence_list = list(skbio.read('test.fa', format='fasta', constructor=Protein))[:args.n_sequences]\n",
    "#print(sequence_list[0].sequence)\n",
    "loaded_proteins = lambda sequence: load_protein_t5_embedding(sequence, args.model_name, args.tokenizer_name)\n",
    "\n",
    "\n",
    "embed_list = (loaded_proteins(x) for x in sequence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el = list(embed_list)\n",
    "print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbatra6/miniforge3/envs/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#embed_list = map(loaded_proteins, sequence_list)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Issue: Generator should include iterables\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mskbio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbagel.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#test if the file was written correctly and output\u001b[39;00m\n\u001b[1;32m      6\u001b[0m read_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(skbio\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbagel.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed\u001b[39m\u001b[38;5;124m'\u001b[39m ,constructor\u001b[38;5;241m=\u001b[39mProteinEmbedding))\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/skbio/io/registry.py:1183\u001b[0m, in \u001b[0;36mwrite\u001b[0;34m(obj, format, into, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(IORegistry\u001b[38;5;241m.\u001b[39mwrite)\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(obj, \u001b[38;5;28mformat\u001b[39m, into, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write data to file using specified format.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/skbio/io/registry.py:616\u001b[0m, in \u001b[0;36mIORegistry.write\u001b[0;34m(self, obj, format, into, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnrecognizedFormatError(\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot write \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m into \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, no \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m writer found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mformat\u001b[39m, into, obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    614\u001b[0m     )\n\u001b[0;32m--> 616\u001b[0m \u001b[43mwriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m into\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/skbio/io/registry.py:1095\u001b[0m, in \u001b[0;36mFormat.writer.<locals>.decorator.<locals>.wrapped_writer\u001b[0;34m(obj, file, encoding, newline, **kwargs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m open_files(files, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mio_kwargs) \u001b[38;5;28;01mas\u001b[39;00m fhs:\n\u001b[1;32m   1094\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mzip\u001b[39m(file_keys, fhs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mwriter_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfhs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/skbio/io/format/embed.py:319\u001b[0m, in \u001b[0;36m_generator_to_embed\u001b[0;34m(objs, fh)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@embed\u001b[39m\u001b[38;5;241m.\u001b[39mwriter(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generator_to_embed\u001b[39m(objs, fh):\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_objects_to_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfh\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/skbio/io/format/embed.py:210\u001b[0m, in \u001b[0;36m_objects_to_embed\u001b[0;34m(objs, fh, include_embedding_pointer)\u001b[0m\n\u001b[1;32m    208\u001b[0m resize_by \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.38\u001b[39m\n\u001b[1;32m    209\u001b[0m resize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# store string representation of the object\u001b[39;49;00m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# that will serve as an identifier for the entire object.\u001b[39;49;00m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# for sequences, this could be the sequence itself\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# for molecules, this could be the SMILES string.\u001b[39;49;00m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The entries in this string representation can be used\u001b[39;49;00m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# to index the row vectors in the embedding.\u001b[39;49;00m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# For sequences, this is the positional index of the sequence.\u001b[39;49;00m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# For molecules, this is the position index of atoms in the SMILES string.\u001b[39;49;00m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Store the embedding itself. We are assuming that the\u001b[39;49;00m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# embbedding is a 2D numpy array\u001b[39;49;00m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#sequence_list = list(skbio.read('test.fa', format='fasta', constructor=Protein))[:args.n_sequences]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(sequence_list[0].sequence)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loaded_proteins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m sequence: load_protein_t5_embedding(sequence, args\u001b[38;5;241m.\u001b[39mmodel_name, args\u001b[38;5;241m.\u001b[39mtokenizer_name)\n\u001b[0;32m---> 18\u001b[0m embed_list \u001b[38;5;241m=\u001b[39m (\u001b[43mloaded_proteins\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sequence_list)\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     12\u001b[0m sequence_list \u001b[38;5;241m=\u001b[39m skbio\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagel.fa\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfasta\u001b[39m\u001b[38;5;124m'\u001b[39m, constructor\u001b[38;5;241m=\u001b[39mProtein)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#sequence_list = list(skbio.read('test.fa', format='fasta', constructor=Protein))[:args.n_sequences]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(sequence_list[0].sequence)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loaded_proteins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m sequence: \u001b[43mload_protein_t5_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m embed_list \u001b[38;5;241m=\u001b[39m (loaded_proteins(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sequence_list)\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36mload_protein_t5_embedding\u001b[0;34m(sequence, model_name, tokenizer_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# (In case we want to use ONNX model)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from optimum.onnxruntime import ORTModel\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_name)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5EncoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# convert sequence to formatted list of strings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m seq_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:3754\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3745\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3747\u001b[0m     (\n\u001b[1;32m   3748\u001b[0m         model,\n\u001b[1;32m   3749\u001b[0m         missing_keys,\n\u001b[1;32m   3750\u001b[0m         unexpected_keys,\n\u001b[1;32m   3751\u001b[0m         mismatched_keys,\n\u001b[1;32m   3752\u001b[0m         offload_index,\n\u001b[1;32m   3753\u001b[0m         error_msgs,\n\u001b[0;32m-> 3754\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3761\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3765\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3774\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:4162\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4143\u001b[0m         error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4144\u001b[0m             model_to_load,\n\u001b[1;32m   4145\u001b[0m             state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4158\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4159\u001b[0m         )\n\u001b[1;32m   4160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4161\u001b[0m         \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[0;32m-> 4162\u001b[0m         error_msgs \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4165\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n\u001b[1;32m   4166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:703\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 703\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:701\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:701\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 701 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:701\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:697\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    695\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 697\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:2075\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2074\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2075\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2077\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#embed_list = map(loaded_proteins, sequence_list)\n",
    "# Issue: Generator should include iterables\n",
    "skbio.write(embed_list, format='embed', into=\"bagel.h5\")\n",
    "\n",
    "#test if the file was written correctly and output\n",
    "read_embed = iter(skbio.read(\"bagel.h5\", format='embed' ,constructor=ProteinEmbedding))\n",
    "for item in read_embed:\n",
    "    print(item.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing to file\n",
    "\n",
    "Finally, we can output the embeddings into a \"test.h5\" file, which can be utilized further\n",
    "as will be demonstrated in other scikit-bio tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Modify the default values of the arguments to match the desired values\n",
    "parser.add_argument(\"--n_sequences\", type=int, default=20)\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Rostlab/prot_t5_xl_uniref50\")\n",
    "parser.add_argument(\"--tokenizer_name\", type=str, default=\"Rostlab/prot_t5_xl_uniref50\")\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# Parse bagel.fa\n",
    "sequence_list = ReadFastaFile(\"bagel.fa\", args.n_sequences)\n",
    "embed_list = []\n",
    "print(\"Accepted protein sequences: \", sequence_list, \"\\n\")\n",
    "\n",
    "#sequence_list = tqdm(skbio.read('bagel.fa', format='fasta', constructor=Protein))\n",
    "#print(sequence_list)\n",
    "#embed_list = list(map(lambda sequence: load_protein_t5_embedding(str(sequence), args.model_name, args.tokenizer_name), sequence_list))\n",
    "\n",
    "# Embed the random/inputted protein sequence(s)\n",
    "for sequence in tqdm(sequence_list):\n",
    "    print(\"Accepted protein sequence: \", sequence, \"\\n\")\n",
    "    test_embed = load_protein_t5_embedding(sequence, args.model_name, args.tokenizer_name).numpy()\n",
    "    #reshape embeddings to fit the skbio format\n",
    "    embed_list.append(test_embed.reshape(test_embed.shape[0], -1))\n",
    "\n",
    "# cast embeddings to ProteinEmbedding object and reshape to fit hdf5 format\n",
    "embedding_sequence_list = [(embedding, sequence) for embedding, sequence in zip(embed_list, sequence_list)]\n",
    "embedding_repr = lambda x: ProteinEmbedding(*x)\n",
    "embed_objs = (x for x in map(embedding_repr, embedding_sequence_list))\n",
    "skbio.write(embed_objs, format='embed', into=\"bagel.h5\")\n",
    "\n",
    "#test if the file was written correctly and output\n",
    "read_embed = iter(skbio.read(\"bagel.h5\", format='embed' ,constructor=ProteinEmbedding))\n",
    "for item in read_embed:\n",
    "    print(item.embedding)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
