{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing ProteinEmbedding and Dataset formation\n",
    "\n",
    "The _ProteinEmbedding_ object can store protein sequences in combination with embedding and vector representations,\n",
    "which can then be manipulated by implementing standard operations from the __skbio__ library. \n",
    "\n",
    "To demonstrate this process, we will read in a list of protein sequences, then attempt to create a dataset from our \n",
    "embeddings to be streamed in with the standard skbio.read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from skbio.embedding import ProteinEmbedding\n",
    "from skbio.sequence import Protein\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import skbio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein Sequences\n",
    "\n",
    "We will now create a function to generate a list of random proteins (of size args.n_prots) to be embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_proteins(n_prots):\n",
    "    import numpy as np\n",
    "    PROTEIN_ALPHABET = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    np.random.seed(42)\n",
    "    proteins = []\n",
    "    for _ in range(n_prots):\n",
    "        prot = \"\".join(\n",
    "            np.random.choice(list(PROTEIN_ALPHABET),\n",
    "                             size=np.random.randint(20, 100)))\n",
    "        proteins.append(prot)\n",
    "    return proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading FASTA File\n",
    "\n",
    "_Bagel.fa_ is a file containing bacteriocin sequences stored in FASTA format. We will parse this\n",
    "file and use the output to create our __ProteinEmbedding__ object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings\n",
    "\n",
    "This function will take the inputted protein sequences and feed it through an embedding model (prot-t5), \n",
    "outputting the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from skbio.embedding import ProteinEmbedding\n",
    "\n",
    "def load_protein_t5_embedding(sequence, model_name, tokenizer_name):\n",
    "    # (In case we want to use ONNX model)\n",
    "    # from optimum.onnxruntime import ORTModel\n",
    "    tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
    "    model = T5EncoderModel.from_pretrained(model_name)\n",
    "    # tokenize sequences and pad up to the longest sequence in the batch\n",
    "    ids = tokenizer.batch_encode_plus(sequence, add_special_tokens=True, padding=\"longest\")\n",
    "    input_ids = torch.tensor(ids['input_ids'])\n",
    "    attention_mask = torch.tensor(ids['attention_mask'])\n",
    "    # generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    emb = embedding_repr.last_hidden_state[:, 0, :].squeeze()\n",
    "    return ProteinEmbedding(emb, sequence)\n",
    "\n",
    "\n",
    "def to_embeddings(sequences : list, model_name, tokenizer_name):\n",
    "    # Embed the random/inputted protein sequence(s)\n",
    "    for sequence in tqdm(sequence_list):\n",
    "        test_embed = load_protein_t5_embedding(str(sequence), model_name, tokenizer_name)\n",
    "        #reshape embeddings to fit the skbio format\n",
    "        yield test_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing to file\n",
    "\n",
    "Finally, we can output the embeddings into a \"test.h5\" file, which can be utilized further\n",
    "as will be demonstrated in other scikit-bio tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:07,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:14,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:28,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 1024])\n",
      "torch.Size([40, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:34,  6.80s/it]"
     ]
    }
   ],
   "source": [
    "model_name = \"Rostlab/prot_t5_xl_uniref50\"\n",
    "tokenizer_name = \"Rostlab/prot_t5_xl_uniref50\"\n",
    "\n",
    "# Parse bagel.fa\n",
    "sequence_list = skbio.io.read(\"bagel.fa\", format='fasta')\n",
    "embed_list = to_embeddings(sequence_list, args.model_name, args.tokenizer_name)\n",
    "skbio.write(embed_list, format='embed', into=\"bagel.h5\")\n",
    "\n",
    "#test if the file was written correctly and output\n",
    "read_embed = iter(skbio.read(\"bagel.h5\", format='embed' ,constructor=ProteinEmbedding))\n",
    "item = next(read_embed)\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
