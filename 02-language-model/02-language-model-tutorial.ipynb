{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Vectors with Scikit-Bio\n",
    "\n",
    "**Welcome to scikit-bio tutorial-02!** In this tutorial, we will showcase how the scikit-bio library can be utilized for embedding and vectorizing sets of protein sequences. Our goal is to demonstrate sequence classification and structural alignment using *TM-Vec* and *DeepBLAST* respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction - a role of deep learning in `scikit-bio`\n",
    "he application of deep learning in biology is gaining widespread popularity, with protein language models (pLMs) being a notable success story. By generating protein embeddings from sequence data, pLMs have paved the way for a range of innovative applications, including, for instance, fluorescent protein design by [ESM3](https://techcrunch.com/2024/06/25/evolutionaryscale-backed-by-amazon-and-nvidia-raises-142m-for-protein-generating-ai/) and [CRISPR-Cas design](https://www.biorxiv.org/content/10.1101/2024.04.22.590591v1).\n",
    "\n",
    "Compared to traditional letter-encoded amino acids, protein embeddings offer a more expressive representation of a protein. In addition to sequence information, they encode structural, evolutionary (including organism of origin), and functional (such as thermostability and fluorescence) properties. These embeddings can be viewed as a compressed representation of multiple sequence alignments (MSAs), and thus, any task that benefits from MSA will also benefit from operations on embeddings. As computational optimizations of protein language models (pLMs) continue to emerge, embeddings-based methods are likely to gain popularity. Our goal is to provide an infrastructure that enables users to conduct sequence analysis leveraging embeddings.\n",
    "\n",
    "However, not all tasks benefit from embeddings. The limitations of embeddings are discussed in detail in [Li et al., 2024](https://www.biorxiv.org/content/10.1101/2024.02.05.578959v2.abstract)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TM-Vec? \n",
    "[TM-Vec](https://www.nature.com/articles/s41587-023-01917-2) is a tool that utilises protein representations from pLM in order to predict structural similarity of two proteins. \n",
    "### How does TM-Vec work? \n",
    "General protein representations (also called \"embeddings\"), that are predictive of protein structure, are obtained from pLM (ProtT5). Next, they are modified into vectors with the help of TM-Vec, which is another neural network. TM-Vec encodes proteins in a way that allows the cosine distance between two vectors to approximate structural similarity (measure via TM-score), thereby eliminating the need for time-consuming structure prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import find_spec\n",
    "if find_spec('skbio') is None:\n",
    "    !pip install -q scikit-bio\n",
    "\n",
    "if find_spec('tmvec') is None:\n",
    "    !pip install -q git+https://github.com/valentynbez/tmvec.git\n",
    "\n",
    "if find_spec('deepblast') is None:\n",
    "    !pip install -q deepblast\n",
    "\n",
    "if find_spec('onnx') is None:\n",
    "    !pip install -q onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skbio\n",
    "skbio.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "!mkdir data\n",
    "!wget -q -O ./data/bagel.tar.gz \"https://www.dropbox.com/scl/fi/uytgo1cw8i65rnht0rzfx/bagel.tar.gz?rlkey=6smjeoy4p0r4b86a6h43ntkqb&dl=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxf ./data/bagel.tar.gz -C ./data\n",
    "!mv ./data/bagel/* ./data\n",
    "!rm -r ./data/bagel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Necessary imports\n",
    "\n",
    "from deepblast.dataset.utils import get_sequence, pack_sequences, revstate_f\n",
    "from skbio.embedding import ProteinEmbedding, ProteinVector\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from skbio.alignment import PairAlignPath\n",
    "from deepblast.utils import load_model\n",
    "from skbio.sequence import Protein\n",
    "import matplotlib.pyplot as plt\n",
    "import skbio.embedding as emb\n",
    "from skbio.io import read\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Embedding Sequences\n",
    "def load_protein_t5_embedding(sequence, model_name, tokenizer_name):\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
    "    model = T5EncoderModel.from_pretrained(model_name)\n",
    "\n",
    "    # convert sequence to formatted list of strings\n",
    "    seq_list = []\n",
    "    seq_list.append(sequence)\n",
    "    seqs = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", str(seq)))) for seq in seq_list]\n",
    "\n",
    "    # tokenize sequences and pad up to the longest sequence in the batch\n",
    "    ids = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "    input_ids = torch.tensor(ids['input_ids'])\n",
    "    attention_mask = torch.tensor(ids['attention_mask'])\n",
    "\n",
    "    # generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    emb = embedding_repr.last_hidden_state[0, :-1, :].squeeze().detach().cpu().numpy()\n",
    "\n",
    "    return ProteinEmbedding(emb, sequence)\n",
    "\n",
    "\n",
    "def to_embeddings(sequences : list, model_name, tokenizer_name):\n",
    "    # Embed the random/inputted protein sequence(s)\n",
    "    for sequence in tqdm(sequences):\n",
    "        test_embed = load_protein_t5_embedding(str(sequence), model_name, tokenizer_name)\n",
    "        #reshape embeddings to fit the skbio format\n",
    "        yield test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load_vector function for vectors\n",
    "def load_vectors(file_path, sequence_list : list):\n",
    "      data = np.load(file_path)\n",
    "      vectors = data['embeddings']\n",
    "\n",
    "      protein_vectors = [ProteinVector(vector, sequence) for vector, sequence in zip(vectors, sequence_list)]\n",
    "      return protein_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural Alignment Helper Functions\n",
    "def condense_cigar(cigar_str):\n",
    "    \"\"\"\n",
    "    Convert full length to condensed CIGAR\n",
    "    Example: MMMIIII = 3M4I\n",
    "    \"\"\"\n",
    "    condensed_cigar = ''\n",
    "    current_state = ''\n",
    "    count = 0\n",
    "    for i in range(len(cigar_str)):\n",
    "        if cigar_str[i] == current_state:\n",
    "            count += 1\n",
    "        else:\n",
    "            if count > 0:\n",
    "                condensed_cigar += str(count) + current_state\n",
    "            current_state = cigar_str[i]\n",
    "            count = 1\n",
    "    condensed_cigar += str(count) + current_state\n",
    "    return condensed_cigar\n",
    "\n",
    "\n",
    "def tm_to_cigar(tm_alignment_string, condensed=False):\n",
    "    \"\"\"\n",
    "    Convert TMalign style alignment string to CIGAR string\n",
    "    \"\"\"\n",
    "\n",
    "    cigar = ''\n",
    "\n",
    "    for state in tm_alignment_string:\n",
    "        if state == ':':\n",
    "            cigar += 'M'\n",
    "        elif state == '1':\n",
    "            cigar += 'I'\n",
    "        elif state == '2':\n",
    "            cigar += 'D'\n",
    "\n",
    "    return cigar\n",
    "\n",
    "def align(x, y, model):\n",
    "    pred_alignment = model.align(str(x), str(y))\n",
    "    # TODO : need to convert TMalign style string to cigar\n",
    "    cigar = tm_to_cigar(pred_alignment)\n",
    "    cigar = condense_cigar(cigar)\n",
    "    path = PairAlignPath.from_cigar(cigar)\n",
    "    return path\n",
    "\n",
    "\n",
    "def predict_aln_matrix(query_seq, target_seq, model):\n",
    "    x_code = get_sequence(str(query_seq), model.tokenizer)[0].to(model.device)\n",
    "    y_code = get_sequence(str(target_seq), model.tokenizer)[0].to(model.device)\n",
    "    seq, order = pack_sequences([x_code], [y_code])\n",
    "    with torch.no_grad():\n",
    "        gen = model.aligner.traceback(seq, order)\n",
    "    _, aln_mat = next(gen)\n",
    "\n",
    "    return aln_mat.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Embedding sequences to file\n",
    "\n",
    "We're going to start by reading in the bacteriocin sequences with `skbio.read`, then storing the embedded sequences in `ProteinEmbedding` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Rostlab/prot_t5_xl_uniref50\"\n",
    "tokenizer_name = \"Rostlab/prot_t5_xl_uniref50\"\n",
    "\n",
    "# Parse bagel.fa\n",
    "sequence_list = read(\"data/pdb_hits.fa\", format='fasta')\n",
    "embed_list = to_embeddings(sequence_list, model_name, tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(embed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Building vector-DB and plot Ordination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly feed our FASTA file into the tmvec build_db __CLI__ function, which will output our \n",
    "vectors as a .npz file in the specified directory.\n",
    "\n",
    "This function takes in as an input:\n",
    "1. --input-fasta: A FASTA file containing your sequences.\n",
    "2. --output: the file location to output to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tmvec build-db --input-fasta data/pdb_hits.fa --output outputs/pdb_hits_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with our DB in hand, we can use the search __CLI__ function to search for proteins against our database, and return the k-nearest neighbors results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tmvec search --input-fasta data/pdb_hits.fa --database outputs/pdb_hits_fasta.npz --output results/pdb_hits_search_results --output-fmt skbio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the ordination results\n",
    "\n",
    "Finally, we can utilize the skbio embed_vec_to_ordination function to create ordination objects from our ProteinVector objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequences\n",
    "sequence_list = read(\"data/pdb_hits.fa\", format='fasta')\n",
    "\n",
    "#read in vectors to generator object\n",
    "vec_generator = load_vectors(\"outputs/pdb_hits_fasta.npz\", sequence_list)\n",
    "\n",
    "ord_results = emb.embed_vec_to_ordination(vec_generator)\n",
    "\n",
    "df = pd.read_csv(\"data/bacteriocin.csv\")\n",
    "df = df.dropna(subset=['Sequence']).set_index('Sequence')\n",
    "df = df.groupby('Sequence').first()\n",
    "\n",
    "common_ids = list(set(ord_results.samples.index) & set(df.index))\n",
    "\n",
    "df = df.loc[common_ids]\n",
    "ord_results.samples = ord_results.samples.loc[common_ids]\n",
    "\n",
    "# plot the results\n",
    "ord_results.plot(df, column='class', title='Bacteriocin Sequence TM-Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Structural Alignment with DeepBLAST\n",
    "\n",
    "DeepBLAST is a deep learning tool that leverages protein embeddings to incorporate structural alignment information into sequence alignment. In essence, DeepBLAST has distilled information from thousands of observed protein structure alignments and integrated it into its alignment matrix. Consequently, DeepBLAST alignments can accurately match dissimilar amino acids based on structural information, making it the most accurate sequence-based method in the [Malidup and Malisam benchmarks](https://www.nature.com/articles/s41587-023-01917-2/tables/2). hile it falls short of structure alignment methods in terms of accuracy, DeepBLAST significantly reduces computational requirements, rendering the analysis of large datasets more feasible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model from scikit-bio HuggingFace repository!\n",
    "!wget https://huggingface.co/scikit-bio/deepblast/resolve/main/deepblast-v3.ckpt -O ./data/deepblast-v3.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropbox backup\n",
    "# https://www.dropbox.com/scl/fi/gvbfgn7rnn4fqd2qq6fng/deepblast-v3.ckpt?rlkey=z7aig3kj1jjtt2q6qlu065afw&dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagel_list = read(\"data/bagel.fa\", format='fasta', constructor=Protein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(sequence_list)\n",
    "y = next(sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./data/deepblast-v3.ckpt\", device=\"cpu\",\n",
    "                   alignment_mode=\"smith-waterman\"\n",
    "                   )\n",
    "\n",
    "path = align(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Visualization of Predicted Alignment Matrix\n",
    "\n",
    "Probabilities and regions of alignment between two sequences can be visualized, thus enabling detailed analysis of conserved regions of proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = predict_aln_matrix(x, y, model)\n",
    "\n",
    "# visualise matrix with cbar\n",
    "plt.imshow(matrix, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
